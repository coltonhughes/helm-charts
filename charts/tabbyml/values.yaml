replicaCount: 1

deploymentStrategy: {}

image:
  repository: tabbyml/tabby
  pullPolicy: IfNotPresent
  tag: 0.30.2

imagePullSecrets: []
nameOverride: ""
fullnameOverride: ""

models:
  code: CodeQwen-7B

container:
  args:
    - serve
    - --model
    - "{{ .Values.models.code | default \"StarCoder-1B\" }}"
    - --chat-model
    - "{{ .Values.models.chat | default \"Qwen2-1.5B-Instruct\" }}"
    - --device
    - cuda

gpu:
  enabled: true
  type: nvidia
  resources:
    requests:
      nvidia.com/gpu: 1
    limits:
      nvidia.com/gpu: 1

serviceAccount:
  create: true
  automount: true
  annotations: {}
  name: ""

podAnnotations: {}
podLabels: {}

podSecurityContext: {}
securityContext: {}

service:
  type: ClusterIP
  port: 8080

ingress:
  enabled: true
  className: nginx
  annotations: {}
  hosts:
    - host: tabbyml.lab.articops.com
      paths:
        - path: /
          pathType: ImplementationSpecific
  tls: []

autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 100
  targetCPUUtilizationPercentage: 80

persistence:
  storageClass: ""
  existingClaim: ""
  enabled: true
  accessMode: ReadWriteOnce
  size: 20Gi
  mountPath: /data
  name: data

volumes: []
volumeMounts: []

nodeSelector: {}

tolerations:
  - key: "nvidia.com/gpu"
    operator: "Equal"
    value: "present"
    effect: "NoSchedule"

affinity: {}
